All right, and we're live. Let's maybe wait like a couple of minutes, see if anybody actually jumps on and then we can jump into it. Yeah. A couple of people live streaming through Eugene's life right now is like the day in the life of Amazon. Tick tock. Exactly. We got we got, you know, 30, 40 views and then five, 10 people on YouTube. So yeah, for anyone who's like, for the first time, you know, we try to do these every once in a while just to catch up on what we're working on, conversations, share any news that might be interesting or relevant. And this is my way of avoiding having to read all the tweets and really get the highest signal information from both Pamu and Eugene. And maybe we can just do a quick introduction of all three of us, kick that off for the first one of the year. And then we can just, I don't know, I have some stuff I want to talk about, but I'm also very curious about what Eugene has been has been up to. So I'm Jason.  There's really no host. I'm just organizing this week. And most of what I've been doing these past couple of months has just been thinking about retrieval, thinking about RAG, structured outputs, a little bit of investing. And yeah, I'm really thinking about what to do for the rest of the year. So yeah, let's jump into maybe Hamel and Eugene. Okay, I'll go first. I'm Hamel. I do consulting like Jason. I do a lot of work for Answer.ai right now. So that's a R&D lab run by Jeremy Howard, Eric Reese. Hi, I'm Eugene. I sell books at the bookstore. So very specifically, I work on recommendation systems and search at Amazon Books. Opinions my own, of course. More recently, I've also been dabbling with generative AI in terms of how we can improve the reader experience. So in this year, I'm thinking a lot more about how to build product that's AI-infused, but doesn't distract from the reading experience. So that's one of the more recent write-ups I shared about AIReadingClub.com, which is a prototype for me to think this through. I also, this year, I'm also, I think the time is ripe, a little bit more ripe for to think about recommendations. They are augmented with LLMs, either using LLMs to read out irrelevant recommendations or using LLMs to augment co-start. So there's a lot of literature I'm combing through in 2024, last year, to try to see where the state of that is there right now and how we can apply it to serve customers better. I have one question, Eugene. So you mentioned, you know, this year it's about having Gen-AI in the product without distracting it. Like, was there data to suggest that most things were distracting last year? I think every year, it's always, we want it to be in the product without distracting. So maybe I can share a little bit more context. So I think there's this tweet by Andrej Kapati where he was saying that he wants the perfect use case for him is, you know, writing, sharing about reading books with an LLM. And then one of the tweets that was a response was this tweet by Patrick Collison. So Patrick Collison was saying that, you know, this is currently right now, it's a pain to do it, but he does it anyway. And the way he does it is that he gets the PDF from somewhere, uploads the PDF to JGBT or Claude. And then that's how he has it talk to the PDF. From my perspective, I find it very difficult, right? Because you're asking questions of the PDF, but you don't actually have the PDF in the context. It's really hard for you to understand, hey, now, where is it coming from? Where is it getting the information from? So that's when the AI is. is only the AI, it's only chat. And, you know, context is a secondary priority. It's just a side dish, side loader where it's retrieval. On the other hand, imagine it where the PDF is the main thing, you're scrolling through the PDF, you're highlighting it, you're asking questions, a side pane pops up, okay, you're done, the side pane pops back up, you focus on the PDF. And that's what my prototype that I was doing, to try and see, okay, can we build such an experience to try and see this? I'm not a product designer, I'm not a product guy in the first at all, but I was just testing, hey, I think the technical problems are largely solved there. Now it's trying to figure out what the right design is for the customer. Yeah. Have you guys seen any new research in the past couple of months that has been exciting? I've been kind of on vacation for two months now. So have I, man. I have not, but I've been looking at a lot of the REXIS papers. I really use LMS a lot, but what about you? you have on? I'm excited about modern BERT actually. So, uh, like you're using that for all kinds of things, especially like retrieval and embedding things and things like that. I really like small models, you know, they're exciting. But where is modern BERT being used now? Like, could I use it as not only an embedding model, but also doing like query expansion and like, like query understanding or I don't really know the limitations of those models are these days. Hmm, that's a good question. Like, I'm still using it. I'm still I haven't gotten too far and using it myself. So I haven't, you'd have to ask like someone else, like Ben, or there's two Ben's there's a Benjamin and Ben. I'm getting started on like a code retrieval thing I'm going deep on. But I haven't got to the modern BERT There's been a couple of investors that have been like, Jason, have you seen this new article that came out? It's great. It's about these guys testing out Devon. Oh, yeah, yeah. Yeah, I'm curious if you'd like to share a little bit more about like that as well, because I have not had time to read it. But I know that like OpenAI is thinking about doing like, more code agents, right? Yeah, raise like a series B. So the thing is, like about Devon is like, there's a lot of hype around it, but then I couldn't find anyone, I couldn't find like detailed accounts of people using it. Like, hey, I use it for XYZ, real tasks, like at work, or try to use it for some extended period of time. There's definitely a lot of, you know, there's some accounts like dismissing it completely. And then there's other accounts, there's plenty of accounts saying, oh, it's amazing, it's going to change the world, software development is going to go away, blah, blah. of that stuff, which it's like, I don't know, it's some kind of different flavor of hype that I haven't seen before. So it kind of, we were like, let's look into it because like, I don't know, let's just find out for ourselves because we don't know. So we tried to use it at Answer.ai, like seriously for a month. And actually it was like, I got interested in it because you were using it, Jason. And I was like, okay, it was kind of funny like watching you use it. Cause you would use it and you would be like, cause you invited me to your Slack. And so I could kind of watch it. And it was like really entertaining. It looked like fun kind of. Cause yeah, so I was like, okay, it sounds like it's neat. Eugene, you have not tried Devon, right? I have not. I have not. Okay, okay. So the UX is like really nice. It's like, you just chat with it in Slack and it chats back to you. It's like, okay, I'm going to go do that. And it goes. and does a bunch of things and it reports back to you, ask you a question. It's just like fun just to try it. And so, yeah, so like I tried it, it worked really good for like the first task I've put it through. Maybe it's just dumb luck. They're like, okay, let's really give it a go. Let's try it on like for a bunch of different things that we need to do. And then kind of, it didn't work for most of the things. And then we said, okay, like, you know, let's just write up what we did and share it. Like, what are all the things we tried and what didn't work? And the thing that was kind of disappointing is there was, we couldn't figure out a pattern of what worked and what didn't. Like it just failed all over the place and it succeeded in some places, but we weren't like, oh, it succeeded there because of this. And like, maybe if you use Devon for like X, Y, Z, but like, we couldn't figure that out. Like. It's like failing at random versus. is like there's some segment of inputs that are... Yeah, it wasn't like, oh, it's like really good for this kind of thing. It's gonna nail it. I can kind of guess maybe what it might be better at. We didn't try everything under the sun, but whatever is like in our workflow. I will say that answer has a little bit of idiosyncratic tools and workflows and ways of working and all that stuff. But like, whatever, I'm sure like a lot of people have idiosyncratic things. So we like, yeah, we just wrote that up and shared it. And it got a lot of attention, which was super surprising to me because I thought, no. I almost was like, I was kind of like lobbying to not publish anything. I was like, who cares? I thought who cares? Like, but whatever. But Devin is hypey. Yeah, and you did try it for 20 use cases and the success rate was surprisingly, unfortunately low. I guess the question is with more hand-holding, do you think it could do better? I think, like you mentioned at the end, right? It's like, oh, you know, a developer using a cursor framework whereby there's maybe more human-in-the-loop interjection on, hey, this is what I want, this is not what I want. It actually works better. Whereas Devon is mostly hands off the wheel, maybe two hands off the wheel. Do you think that's the issue? Yeah. How could we get it to work? It's hard to see. I don't know, cursor just feels Pareto optimal. It's just really good. Maybe it's Pareto optimal for people who already know how to code. Maybe, but I don't know. They have the composer, you know? This is pretty decent. So I just couldn't see, like, OK, it couldn't understand the situation where I thought, OK, I want to use this. Maybe it's there for somebody, but because cursor is there. Fair. I guess another thing I'm curious about is, have you used the O1 Pro model? for coding, because I have not used it for coding at all, but I use it for a ton of creating worksheets for my lectures. Like, it's definitely paid itself off, but it is not what I had expected to use it for. A lot of folks have been telling me to use O1 Pro, and I was asking them, yeah, exactly, what is it that has been useful? And then a lot of times they mentioned coding. Okay, okay, I mean, yeah, other than coding where there's deterministic outputs where you can evaluate it, what else have you found useful? One guy said that he's been finding it useful as a career coach, like consulting, like consulting is like, oh, what do I want in life? Like, where do I, here's where I want to see, how do I get there? And your idea of worksheets really reminds me of that, whereby it helps you build a scaffold for the human mind to think through. Yeah, like what I do is I give it, like one thing that O1 Pro is very good at, I'll give it like 12 hours of office hours. Yeah. say, give me every question answer pair. And for every answer, give me my answer, word for word, cleaned up a little bit for grammar. If you do it for SONNET, you'll get like 12 questions because it just can't output that much length. For O1, I'll think for like seven minutes and then I'll capture like all 68 questions that I had mentioned in 12 hours of Office Hours. So what you're saying is that O1 Pro is an amazing ETL engine. Yeah. So does it essentially do the work that doc ETL is supposed to do? If you're familiar with doc ETL. Potentially, potentially, right? And the fact that it's a hundred thousand token output, like I've asked for it to give me a like a 40 page, 20 page, tiny PDF, like ebook. Like here's my entire lecture, give me like an ebook that I can make for free at my... questions. So for coding, there's like a couple of people. There's that guy, Matthew McKay. I think I'm pronouncing his name correctly. And then there's another person and they, so there's some, okay. Like one workflow is like to have O1 create a PRD and then in a bunch of other design documents and dump those into cursor as context. Yep. And then Matthew shows something. I haven't used it yet, but he basically takes an entire code base, dumps it into O1 pro like he's trying to do some kind of like major refactor. And then he like starts with, and he has some tool that will take the output of O1 cause he can't, I guess the O1 pro is not API yet. He takes the output and he has like this XML. There's a, there's a prompt and it takes the XML, he parses it and applies it to the code base. And he uses that as a first starting point and then like starts using cursor. from there. I haven't tried the second one. I've tried the first one. What do you think about the first approach? That's the approach I took to build AI Reading Cloud. Yeah. I think like I have met my workflow is so incremental based because of like all the notebook infrastructure stuff at Answer.ai. It just like doesn't, I don't know, like it doesn't work super well. It seems like it does help a lot, like if I'm doing things in the more mainstream way. But even then, I don't know, I don't like to one-shot too many things at once. It's just me. I need to understand what the code is doing. So I like to build one piece at a time and like understand like what it does. So there's... I feel like I'm missing some kind of kind of thing. Yeah, I think I can comfortably let it write like 700 lines of code. I'll never have to edit it again. But if it generates 700 lines of code in a 600,000 line repo, I get very nervous. I got one way that I've been thinking about it recently is like, you know, what does a good tool look like for the L3 engineer versus the L4 engineer versus the L5 engineer versus the L6 engineer versus the L7 engineer? And thinking about those all are a different suite of tools. In each of those levels of engineering, you're also willing to pay a different amount of money for their product. Like an L3 engineer is very willing to pay for cursor, whereas I personally would love a $2,000 model that would actually work. And if OpenAI said, give us $1,000 of test time compute, we'll give you an e-book based off of the $30,000. hours of lectures you've ever given about this topic that's it's free you know i want to make my money back um but i'm curious like what do you think are the different tools that like the alchemy engineer needs versus what like the l6 engineer needs like they're not the same thing right yeah i guess what i can say is i think the time is so recently i've been chatting with a few um very senior managers or director and that they think right and these are engineering folks they think that the time is ripe for them to go back to become an ic because the tooling is able to provide them with so much leverage that they can take the design in their head the customer problem and very quickly build a prototype and even get maybe 80% of the way there to production. So I think the tooling is very much right. Yes, there could be more, there could be like, oh, you know, spin up my AWS infrastructure, handle all the permissions automatically, build the data pipelines. Building data pipelines is still really hard for LLMs. Figure out how to build data pipelines, track the data pipelines, everything. But I think it's, I'm very grateful for what we have now. I think it's, it really allows you to run very fast. So for example, like the AI reading club thing, right? The prototype, like people ask me, like people in my, how long did it take to build this? Oh, it just took me like two, three hours a day for like five days a week. So it's a 10 day thing. It's a two work day thing. And it's like, yeah, it's like, how do you do it? It's like, it's just Next.js and cursor. I think people can run really fast right now with the tools. but what are the gaps? Devin, let's do a gap. You know, like, I feel like the way you use it should be different. Maybe, like, it's hard to say, like, because I try to act like a junior engineer sometimes, myself, because, like, I don't know everything. And so, like, I, you know, I like to use it for learning a lot. Like, hey, explain this to me. What are different ways that you can implement this? What are the pros and cons of that? What are some alternatives? Whatever. Like, I get, the way I talk to, you know, AI, I talk to it like a beginner, and then sometimes talk to it like I already know it, whatever. So, I don't know, I think a large part of it is, like, using it in the right way, differently. Um, I just think, yeah, I mean, I don't know. Cursor is really good, though. It's like, I don't. I feel like it just spans so much breadth. Like, you can use it at, like, a micro level. It can, you know, use Composer or more, like, Agent, whatever. I don't know. So what's your answer, Jason? Like, why did you get started thinking about this? No, just the fact that these tools are all different. And I feel like so much of all of what I've been doing now is just, OK, what is the product I want to offer? What is the price point of that offer? And if it went, like, higher or lower, what more can I offer? Right, and I'm kind of seeing what O1 Pro has to offer. And now, OK, like, what does a $2,000 O1 look like? Like, what does a $2,000 O3 look like? You know, if they had O1 plus a cloud project, and they told me that was $500 a month, I feel like I would be happy to pay it. Because I would just have a single. you know, single document of, you know, all of my like 30 hours of lectures. And he was great, like create me a 12, like 12 email newsletter sequence, pay for me a first draft of the ebook, you know, create me the landing page for the ebook. Like I can actually build a product and like there's a world where if there's a powerful enough version of an agent, like I would give it a percentage of revenue because that's what I'm doing right now with the people I'm working with, right. And I'm actually just more and more wishing there were more expensive products that got me closer to getting an actual result. I see. Right. It's like, I'd pay $6,000 for some work, I'd pay $40,000 for some work, right? Yeah, there's a joke. I'd pay $2,000 for someone to make me PowerPoint slides. There's like somewhere I heard that like Sam Altman says, like when they reach AGI, they're gonna make, you know, pay the favor back to their investors. by just asking the AGI to make the money. So, it sounds like almost similar. It's like, hey, you want, you know, the model to help you make money directly. Then we're happy to pay as much as, you know, accordingly, like really high amounts. So, okay. Changing topics a little bit. This is actually the reason I called all of you guys here, which is I'm having a difficult time with a technical problem. I think from the beginning of, like, last year, most of what I had been thinking about in terms of function calling and tool calling was just the precision and recall of how we can put tools into the context and then the precision and recall of what tools are being used. Right? And at that point, we're getting like 80%, 90%. It's doing pretty good. We're finally getting to a place now where I think with something like O1 or enough few-shotting, we're often picking the right tool. And now I think we've earned the ability to start thinking about evaluating the tool arguments, especially very complex arguments. Outside of LLM as a judge, I don't really know how to think about that on a holistic perspective. I can always write custom scoring functions or custom comparison functions. But I am curious what you guys have been thinking about, if at all, right? Like evaluating function calling agents by thinking about tools. I can go first. I have a lot. I've mostly been using fairly basic retrieval. So I'm curious, what kind of arguments are you thinking about? And are there like clearly... good arguments and correct arguments and wrong arguments or is it like a There are some arguments that are better than others. Is it like SQL query is an argument? No, it's like actually following different APIs, right? Maybe the API has like five or six different arguments and parameters somewhere optional You know some of them in the date range some of them is like, you know start and end time But the end time may or may not be null. It's really hard to think about how to evaluate any of that So if a human looked at it, if like one of the engineers actually is familiar with the API, they would know what is the correct answer? Probably, right? You know, but the difference could be a subjective one of I asked for like what's going on this week Did it select a Friday? Did it select seven days? Hmm Yeah, I don't know right like I know like I know errors could be like, you know, you might be selecting inclusive versus exclusive of a single day and that's a property of the API, right? Like one argument could be, if you're going to evaluate the tool call arguments, you may as well be evaluating the entire multi-hop loop, because you actually want to evaluate not only the arguments, but the return arguments, the return results, maybe the full conversation, who knows? But I think we're getting to a point where we know the tool we need to call. And now the question is like, do you do the next step or do you just do the entire conversation as a single test, right? And I just don't have an answer. I personally just don't have a strong opinion on some of these things. Yeah, so like for the function call, the date thing you'd say is funny, because I just have like something I was working on. I'm working on like a real estate, a multifamily real estate agent. I really hate to say the word agent, but whatever it is, just use it. And it does things like it responds to potential residents by giving them booking dates and whatever. And it was always getting the dates wrong. Because it has to call a function. And you can ask like, I mean, I can just show you some examples. Because it's like, I have some fake data that, let me see. Does Domi share my screen? Yeah. I'll put it up. OK, so this is an example of the brain trust. So this is kind of realistic. It's like, you are a leasing team member, blah, blah, blah. You're engaged with the customer, book tours. You are capable of handling various data inputs, blah, blah, blah. Follow these specific rules, whatever. All kinds of stuff, right? And then basically, I went through. the data and did like a bunch of, for this particular error, we're like, okay, it's always getting dates wrong. And then so what I did is like, I outlined all of the different failures, like kinds of genres of failures that would be getting wrong and generate a bunch of synthetic data that would trigger those errors. And then basically like, okay, here, you know, like I have like, oh, you're looking for a three bedroom unit, blah, blah. And I know like, okay, you know, there is a, there is a, there's a ground truth somewhere. I don't know where the hell it is. Oh, yeah. Right. And so like, I know what the outcome is supposed to be. So I guess it's not really the arguments. It is an argument to some degree, like it is like you have to give the right input here. But essentially like, yeah, it just has. like a bunch of test cases and then like whatever so that's like these more deterministic things for the more like and like ambiguous stuff like where there's not a single right answer then that's like kind of similar to the honeycomb stuff i mean it's like a sequel query so it's like right a little bit more loose but i've been using lm as judge but i go through that process of like writing critiques and aligning it yeah it's like almost like if you're gonna go from a precision recall evaluation to something more complex you may as well just jump all the way to the end yeah it was like the right recommendation right like it shouldn't be are the arguments correct have a suite of tests and then it's like okay once you call the function have a suite of tests it's like no no at that point you're just testing the a to b yeah you have to put in the work to like do those critiques or like have lm judge you just have to know like whether you can trust the lm judge that's the only problem so you have to do that work to say like hey like we have this judge, it agrees with me 90% of the time or whatever it is. And like, okay, that's good. We trust it enough. And you can refresh that. But yeah, I wonder if you can simplify at an argument level, come up with criteria on what is right or wrong or good or bad. And then those are the criteria that you should allow them to judge for. Yeah. Cause I think in worlds where there's like five function, I just write custom scores for each one. Right. And it's mostly just like watching for changes, like a snapshot test. Right. It's okay. Well, you ran it with, Oh, one, these are the arguments. How similar are Oh four, four Oh arguments to the Oh one arguments. Right. Um, but like, I, I've been working with companies where do we have like 90 extensions, it's like, okay. Yeah. Right. It's like 20 commands now become very tricky. I think in that case, though, sorry, go ahead. This is very similar to what I'm facing whereby I'm not working on extensions, but it's use cases. And then like maybe this year we're going to go in order of magnitude more use cases and an order of magnitude more marketplaces. The only way I've come to the conclusion and only to scale that is maybe an LME evaluator. And then how we try to organize it is come up with criteria that will apply to most of these use cases. And then we work based on those criteria. Yeah, that makes sense. Yeah, it's funny. I feel like only this year have the people who took last year's ride course been able to then do this segmentation and investment in the actual specialization of certain tasks. Yeah, it takes time. Last year was still too early, basically. Yeah. I'm working a lot on teaching people how to look at data. So like last year, I think I just yelled into the abyss. look at data, look at data, you just left to look at it. But, you know, I realized at some point, like, people don't know what I'm saying. Like, they don't understand. They're like, yeah, look at data. But then they don't really understand. You have an answer yet, Hamel, to that? Yeah. And I have another question for you all after this. Yeah. Yeah, I mean, the answer is like, yeah, there is a way to look at data. There's like a whole process you should go through that's so ingrained in all of you that it's like, you know, it's like breathing. So, you don't know that. Yeah, it's hard to tell you how to start to breathe. But, you know, do this thing on your nose and then go into your lungs. Yeah, exactly. And so, I had to really, like, reflect on it deeply and, like, watch a lot of people fail looking at data to understand, like, okay, this is how you, like, this is how you don't know how to look at data. But I think that's the main problem. Like, doing data analysis. of logs. Basically, how to do data analysis. And I'm not talking about statistics or anything like fancy, just counting. How to count stuff and how to look at stuff that you're counting and where to look and kind of whatever. How to even know if your data smells bad, if your conclusion is even bad, or if your SQL join is wrong. I mean, Albert asks like a really solid question, which is like, can you discuss what bad data looks like? It's not as much as bad data, it's a bad process of looking at data. Because you have to clean your data. But it's like, okay, like systematically look at your data and start like enriching it with your own labels, or like apply facets. There's this term, like you can say apply dimensions, apply facets, whatever. But it's like, you know, segment that data according to like, whatever is interesting to your business. So, features or personas or whatever it might be. And start like doing some basic data analysis on that. People don't know how to do that. And it's not about like bad data. Bad data is like data you can't even look at because it doesn't contain the information you need. So, or data that is just wrong, like, you know. But it's more like the driver that's bad, usually. Because it's your job to make the data good, yeah. I can share a very silly example. Every time I do a data analysis, maybe it's just a habit, I always print out the number of rows left in my data frame. One thing I found is that every time I'm reviewing someone else's methodology, either they do a join that causes the data frame number of rows to explode because it's not on a unique key, or they do a join and then they drop like 80% of the data, unintendedly, right? And therefore it's like either they just have duplicated a small segment of data or they lost a huge segment of data. I think something like that even is. Unless you've made that mistake often enough that it feels painful that you would even bother to pay attention to that. Yeah, I recently asked people about pivot tables for this reason. Do y'all have, both of you know about pivot tables? I barely know how to use it. Okay, all right, so like So like I have a theory like okay So for me, the only way I really feel comfortable with data is if I have like a physical connection to it like I feel like I can It's weird because data is not physical. It's an abstract concept And the way that it gets feels physical. It's like It's like very low latency Exploration of it like oh, I'm looking like turn it this way turn it this way look at this detail and there's no friction at all like while I do that and And You know like pivot tables allow me me to just change my mind really fast, much faster than code. Like, oh, I want to see this here in this way. I want to filter this real fast, or whatever. It just takes two or three seconds longer than writing code, or writing code is two or three seconds longer, and it just adds up. And then little things like pivot tables, you can aggregate your data into a grid, and then you can double click on a value, and then it explodes and shows you what's underneath that. And then you can go and fiddle with it. And you can just keep exploring. And I don't know, this intuitive feeling of the data, I find it to be important for myself to understand, to give me comfort. I know what's in there, and I get it. But then without that, OK, maybe there's other things like pivot tables. Looker, this other Tableau has some pivot tables. It's not unique to Excel, but I don't know, what do you all use to have that feeling? You get either data frames or spark data frames. Yeah, yeah, you have so much data you have to use spark, but yeah, a lot of what I end up doing is pandas in a Jupyter Notebook stack and unstack. Is kind of the pivot table booklet 10 years ago. And you've got 10 years ago, maybe like six years ago. I will use a deep wire. Doesn't look like you want to switch this thing to be over here and then you need to like change the code. So I'll tell you why that's valuable. The thing is, every time I do an operation, it's just a separate cell and it's a documentation log. So I don't lose anything. And the amazing thing about working in Jupyter Notebook is visualization is amazing. I always find more in the visuals than in a table. Yeah. And I do a lot of visualization. I'm a heavy visualization guy. There's another thing though, but sometimes there's ways to do visualization. And like, even with the pivot table concept, like dragging accesses over here and there, whatever. And I just feel frustrated sometimes, like I'm not holding the paintbrush. I'm just like, I'm interacting with this abstract medium and I can't like play. If I want to create an artifact, I love code, nothing beats it. But then it's like, just to play with it, I don't feel. It's more intuitive with pivot tables, yeah. Yeah, I will say, like, I think here, a lot of it is people's inability to actually do like multi-dimensional data visualization. Like, what I want to be able to do is, I want to be able to just create like a grid of plots. And we'll see like, oh, most of the lines go in the same direction. And then one of them is not. Okay, I need to go investigate that subsection of the data. Let me afford this a little bit more. Oh, like, let's see what kind of variables do I have. Let's visualize these variables. Oh, again, it's like, it's all flat. Then like one subsegment is like very low. What's going on there? So it's a bit of, yeah. same thing of being familiar with the data, but I guess for you, it's like you're kind of moving things around. I just want to be shown like 55 plots and visually figure out what is the like discontinuity in my data set. I wish I had an example, but you know. Yeah, I mean, I like that too. There's like that auto EDA stuff that's like just shows you a bunch of shit, like plots, like histogram, every variable, blah, blah, all that data science stuff. It's really hard. Yeah, it generates a lot and unless you know what you're looking for, it's not so straightforward. It's just like, I don't know, I can't describe it. It's like once I start looking at data, I have these like hunches, like, oh, I bet there's something fishy right there. Exactly. How do you teach that? That's the thing. It's hard. It's like you won't know it until you actually look at it and then you slice and dice it and then you find different things that you know. Exactly. You know, a lot of similar being. I had a question for Jason and Hamel, and I think you all face this in your consulting work, and working in stand-ups with technical folks. A lot of times, there's the business problem, right? Maybe the business problem is either MRR or retention rate. And then there's how, and sometimes people are like, okay, there's this business problem, and sometimes, okay, we build some system to solve this business problem, but very often what I'm seeing with a lot of folks I'm talking to is that there's a gap between defining the business problem and the business metric, and defining either the engineering metric or the machine learning objective that the system should optimize. Do you see this as well, or do you always have a, before building a system, you always have very clear engineering metric or machine learning optimization metric that you always know, and then you focus on that? Yeah. The people, I wish that people came to me with such a metric. Yeah, so someone has to define, that's the thing. You are always the one that is defining it. But how do you teach teams to define that? It's almost like teaching teams how to define the eval metrics, right? It's really like, okay, so first of all, I think people are really far away from being able to define their match. I spend most of my time teaching people how to look at data because people can't count right now. I mean, I'm not trying to be inflammatory. I'm just saying people can't count their data and like look at it. So then you need to do that before you give the metrics. Once you start to understand like, okay, these are my problems, this is where it's failing, the metrics kind of become obvious. Emerge, yeah. But that's an eval metric, right? How do you, and then the other thing is how do you map the eval metric to the machine learning optimization metric? So it's like, are you just reducing loss or is it validation loss or perplexity? I mean, Jeremy Howard has a report on this. Yeah, I guess I'll maybe add my thing of just like, hey guys, whatever we come up with, it's not gonna be a good number to use in six months. So really, we just have to make sure that whatever we're measuring, that correlates with the thing that is probably a little bit harder to measure. And how do you make sure it correlates well? Well, if you can't outthink the problem, you just have to start acting, right? You just have to start doing experiments and going, okay, we ran three experiments, our leading metric changed, did our lagging metric change? Right? Sometimes the answer is like, oh shit, you gotta do an A-B test or something. I hope it doesn't have to be like that aggressive. But I feel like if last year was like two vibe days, now like everyone wants to like measure everything and they're just like thinking too much. Yeah. Oh yeah, that's come up in my office hours as well. I think the pendulum is strong a little bit. Yeah. Too much. It happened in my office hours. quite often where it's like oh I read their blog post I you know I annotated like 5,000 rows and I you know have like these 25 metrics and I did did like 2,000 critiques and I have you know all this stuff and what I do I'm analyzing like 25 components yeah it's like gone to I've agreed so gone a little bit too far I think because it's like lossy like how do you it's a new one once I say like yeah if you're not confident about what you're doing you end up doing too much yeah yeah yeah you know I wish that it was like more clarity on that like ought to be completely transparent like most people that I work with are just trying to build AI because they think it will help in some nebulous way, but they don't really have a business, a clear, I mean it's not like, it's hard to articulate some clear business metric per se. I think they know how to build it, I think they know how to Kaggle it, but they don't know how to define what is the Kaggle optimization metric, what is the eval, what is the hidden test split, what is the business problem. I think that part is missing because we just haven't, don't have a Kaggle for that, and maybe it's also because it's fuzzy. Amol, do you want to try to answer this question? Yeah, so like we have millions of records, how do you look at your data? So like, this is interesting, like I always get this question, and it's like, you know, we've been solving this in machine learning for decades, because it's the same issue there. It's really like, take a sample of your data, just like 150, just start with 10. I mean, does it... and like don't kill yourself and start looking at data. You don't need to look at all of it. You'll understand, you'll get, once you, yeah, look at like a hundred, you'll be a way smarter and you'll know what to do. You'll know like, oh, there's errors here. And like, oh, maybe I want to look at more data, like X, Y, Z, because that's really fishy. But you kind of have to trust that that will happen. People are like, oh, that seems like what's gonna happen after you look at a hundred, look at a hundred, right? Yeah, and stratified sampling helps a lot. Like, do you want to sample by customer persona, new customers, regular customers, or is it like by your use case? I think that helps a lot for you to know good stratified sampling. Yeah, I'll add a little bit, which is, you know, the real question is like, do you want to sample at random? Not completely, no. You want to start there maybe to get an intuition what the hell is going on, but then you'd have to have some hypothesis. for sure yeah right and also think you want to sample by um engagement what does this mean it means that okay let's say you have a feature that gets 90 percent of traffic and then there's a feature that has 10 percent of traffic like you probably want to take care of the feature that gets 90 percent traffic and that ties back into jason's comment of like aligning with your business like if you have some business thing that you're lying towards it will drive that engagement thing you're like okay i don't have time to worry about this like this small thing yeah i mean again that's that's the whole thing with the course is it's just like you segment your input data you count the percentage of traffic for each segment you look at the probability you can do that segment well and then you look at how much money you would make if you did that perfectly and then n times p times dollars is how you prioritize your improvements. So Jason, that's a good question. I have a question for you here. Sometimes not everything can be reduced down to money. So let me give you an example. So let's say, assuming you're on an e-commerce website. Of course, the e-commerce, the main thing is conversions or clicks. But sometimes let's say we want to introduce a new product line. It's like the explore, export thing. So how do you think about measuring the benefit of showing products to customers that they may not buy? It may, there may be an informational intent. Like the goal is to inform you of this new product line or the goal is something else, yeah. Well, I mean, that is just, I mean, I would go down like the angle of really what we're saying is E is not a point estimate. P is the distribution, right? And, you know, maybe we should. be ordering it by, you know, bottom confidence interval or something. I see. Okay. But then eventually the measurement is based on dollars, right? So how would you, would you try to dollarify, try to make something into a dollar value like informational needs or diversification needs or? Well, it's like, if we do, if we go down like the multi-armed bandit route, you can still like dollarify your reward, but still have very reasonable. I see. So you exploit. Yep. Right. The issue is like, do you use like, like is P, you know, the number of successes about a number of failures or is P some like better estimate for the lower bound of the probability? And so as you sample more, you know, I think like there's a lot of like bandit literature that you can just borrow from heuristically. That makes sense. I see. Okay. put a dollar value on other things or basically have a common metric and try to bring everything the same quantity. Yeah, I mean, I would love to go. Sometimes to do that, you have to do like interviews with people who use your product. Yeah. But I think if someone comes back to me, so like, well, that's hard. And then why can you not get the interview or does no one use your product? Gotcha. Gotcha. In which case, like, yeah, let's really figure out what you're trying to do. I'm going to share a screen real quick. Yeah, I have a question. So like, okay, I really love this feature on Amazon. It's like, okay, how, you know, what kind of are like, okay, how long do the batteries last? And like, always get you really useful information from this. It's actually really, yeah, like, it's great. But then I wonder, I do wonder when I use it. I'm like, I wonder what's going on. Like, I'm not sure like this increases. decreases my likelihood to buy, it just makes me happier that it's there. So like, how is this person working on this feature, how are they like, how can they possibly tie this to money, even though I like it? A-B test. Okay, so tell me more, like. So you can imagine, so. But is it based on, if you A-B test like, based on what though? Like, because. Personal opinions, personal opinions only, not speaking on behalf of Amazon. You can imagine, right, anytime you have a new feature, you could have a control and a treatment. And the treatment would be showing this feature, and the control would be not showing this feature. But how's the signal come? Because I'm happy, but there's, I'm not expressing like, there's no way that I'm giving that back. Maybe are you buying? Okay, so are you giving it back? So there's a few things here, and I think you're getting at what the question I had, the struggle I have. So if you're happy and you're buying, this exact product in the next 24 hours or 48 hours, we can sort of attribute that, that you know, you interact with this, you purchase it. Now, there's another kind of thing, which is the customer long-term value, like, oh, would you, are you using this feature again? Or are you, how happy are you, how informational this is? Now, that is really hard to measure and quantify. And to be clear, many times I don't buy products because of this, but I'm very happy that it exists. And, you know, to be clear, that is, I think from a product standpoint, that's actually valuable because you stop customers from buying things that they would actually be upset and eventually return. So, but that is, you're right, it's extremely tricky to measure this, very, very difficult, yeah. Yeah. What you really need here is like the founders or to have a culture of like long-term holdouts. Yeah. All right. See you tomorrow. You know what I mean? Yeah. I think the question is, does this impact average order value? Maybe not. But does it impact like lifetime total value of the customer? Probably. Does it impact churn, maybe? But then what is churn defined by? Like not renewing prime? I don't know. That's like a three year long experiment, I think. I'm going to have prime no matter what, basically monopoly as far as I'm concerned in my neighborhood. So again, now that's a different problem. You're right. It's extremely hard. And now they've become the problem because attribution. Like whether Hamill churn or not churn, is it because of this feature or the other feature or something else? As the long term hold up gets longer term, Hamill is going to be exposed to more features. So how do you attribute it? I think there's this amazing quote in reinforcement learning that the only problem in reinforcement learning is reward attribution. Or the  attribute the reward to the right action taken. Yeah, it's a hard problem. So Hamel, how do you like this feature so far? This is excellent. Yeah, I really like it. I use it all the time. I use it almost every single time that I buy something and it's there. I'm like, this is great. And I just ask a whole bunch of questions like, oh, like what is it? I think this is a perfect example of a UX where the AI is not the main thing, but the AI serves a lot of help. My hypothesis is that I have come across a lot of people who have shared feedback that they love this. My hypothesis is that this is great because it helps people save time. I mean, you don't want to command F, you don't want to go through all the reviews or the product description. You just want to ask this and it helps people save time. So yeah, therefore the. But the main thing is still the shopping experience. I'm gonna buy my stuff and get out. And the more you help me save time, the better. Whereas the other, so you can imagine that there could be a metric, right? Which is a session length. How long daily average minutes, for example, just picking something up. As a metric like that could be wrong because yes, people are spending more time on your platform, but that's not what you want. You maybe want that on TikTok where you're at platform, but when people just wanna buy their stuff and get out, that may not be the right metric. I don't know. Who's to say that that search thing is making it longer? It could be making it shorter. Yeah, it could be making it shorter, exactly. So something like lower time spent on a website, people think, oh, it means that your website is crap, people are leaving. But no, you could be more efficient, right? It's just like- That is my genuine question. I wish I could talk to the person at Amazon working on this specific feature and be like, how are you, what's your metric that you are evaluated by? Like, how do you, like, it's just very interesting to me. It is and those are the really really hard questions that yeah those are really hard questions that you just don't learn anyway. I would like to believe that they're not you know they have some they're not like tied to so in this case I would I kind of have I would believe it if they're not tied to any business metric and they're just trying to make sure like this output is good like it's factual and it's good. You can you can think of those as guardrail metrics there'll be guardrail metrics and then there'll be success metrics. Guardrail metrics is like okay okay yeah that many defects and then success metrics would be okay we drove 5% incremental conversion or whatever. Yeah the success metrics I would really want to know I'm like yeah okay that that seems very noisy and I don't know yeah. I wonder if this can be used to like personalized landing pages like maybe like two years from now if I've been using Rufus a lot or is it called Rufus I forget. Yeah it's called Rufus You know, maybe what's happening is like, I don't even need to see this anymore. I just, when I go to a product, it knows what kind of questions I'll ask and it'll just show me the answers to the most important questions for you. You know? Wow, we need a lot of data on the customer level then. Like, why would I have to ask? Maybe, you know. Yeah, but I'm sure like every watch, one of the questions is like, is it waterproof? You know, like. Yeah. At some point, I would rather just tell the watchmaker, people really care if this watch is waterproof. Change the title of your product. Just say waterproof watch. You don't need to answer the question anymore, but we've made you make the product better, you know? Potentially. There's like some fun, like, I guess I've trained. There's a lot of user training. Like, I've trained myself to like look at Amazon, like, okay, it's a remote control car. Like, first I look for, you know, like everyone does this. Like, okay, reviews. Like, what are the ones with the best reviews? Then within those, like between those, like, which ones should I get? Right. But what I'm saying is like, imagine someone who will ask about like battery life. I would just eventually just tell the remote control car company to be like, no, just put six hours battery life rechargeable on the label. It is interesting that, okay, because I was searching within books, maybe that's the problem. Like, why is it returning books? Yeah, that's a funny thought. Oh, you know what I will say? Amazon is like overfitting on the back button now. On the back button? What do you mean? Oh, I don't know if this is like a me thing, but if I click something, then press back immediately. My recommendations change. Is it just you? Is it maybe it's just me then? I believe it. I mean, sponsored posts. Yeah. It was really weird. Ah, I see. Maybe it's an ad thing whereby they could be. they don't want to overcharge someone for impressions. So yeah. Yeah, this grid thing is really nice. Like I often use this grid. It's like no battery life. You know what I mean? Yeah. You can imagine, you know, there'll be some kind of analysis going on at the back end where like, okay, what are the most common queries on this? Or imagine like product. Let's say a lot of people look for, I don't know, Jason Liu mug, just making, look at your data mug and maybe there's a gap in our catalog. I think he has another call. So do I. So I actually don't know if he's going to come back, but. I think he'll come back because it looks like his laptop died or something or something very sudden. But if you've got to go, we can. Yeah, I got to go in one minute. We agree catching up. When are you traveling again, Jason? I'm in SF for a while. I think I'm going to do the course in SF and just like wake up at 7 a.m. My apartment in New York is kind of being delayed. So I'll be in SF in March It's it will be a while, but it's a really late if you come in February I'm traveling and I've made a lot of friends from Twitter this time around Fantastic house party scene is poppin when I go to SF. I'm just so tired from all the social. I just do my own thing Well, it sounds like Eugene's gotta go. So I gotta go at 3. I'll see you guys. Take care Yeah, right. Yeah, man I'll probably jump off in a bit too. Yeah, I've been doing 7 a.m. Meetings. Very very sleepy All right, cool, no problem What else have you been thinking about man, how's the consulting going changing your mind? I'm trying to slow it down a lot spend more time with my kids a lot ever since talking to our friend Rishi and As long as friends like not go so hard this year? Yeah, well, you know, if you got customers in the summer, let me know. Yeah, I think I'm on the same boat. I'm like, man, I think the, it's, yeah, you really, you really start valuing your time more and more every day. Yeah, I'm talking to 20 year olds, like, you'll never understand what I'm saying until you're like 30 or 40 or 50. But man, you are rich. It is so easy to make money. You just like, I am turning down money. But like, I have three weekends in San Francisco. That's great, man. No, I mean, like, yeah, you have to figure out what makes sense. Yeah. Um, exactly. So like, for me, my kids are not young forever, or whatnot. Exactly. But it's definitely worth it. Let us end the Maybe we can keep chatting for a little bit, but I think we'll just cut it off for now. And yeah, next time we'll do more of a Q&A session, but yeah, I'll see you then. Awesome. Take care, man. All right. End stream.
